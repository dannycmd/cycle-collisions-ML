{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from dython.nominal import associations\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from functions import transform_raw_data\n",
    "\n",
    "df = transform_raw_data(path_to_csv=\"stats19CycleCollisions2022.csv\")\n",
    "df = df.drop(columns=[\"accident_index_2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking for values that should be classed as missing\n",
    "for i in zip(df.columns, df.dtypes):\n",
    "    if i[1] == 'object':\n",
    "        print(i[0], \":\\n \", df[i[0]].unique(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardise missing values\n",
    "missing_values = ['Data missing or out of range', 'unknown (self reported)', np.nan, 'Unknown', 'Not known', 'Undefined']\n",
    "df = df.replace({i: \"Missing\" for i in missing_values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column summaries\n",
    "pd.DataFrame([i for i in zip(df.columns, df.dtypes, df.nunique(), ((df == \"Missing\") | (df.isnull())).sum(), 100 * ((df == \"Missing\") | (df.isnull())).mean())], columns=['column', 'dtype', 'nunique', 'missing', 'missing %'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop lsoa variables - too many unique values\n",
    "# keep longitude, latitude, date as I may use these in future to join to extra data, but do not use in model\n",
    "# remove age band variables and keep age variables\n",
    "# remove variables with a high % missing - special_conditions_at_site, carriageway_hazards, skidding_and_overturning, hit_object_in_carriageway, hit_object_off_carriageway, journey_purpose_of_driver\n",
    "# (skidding_and_overturning, hit_object_in_carriageway, hit_object_off_carriageway are also variables that would only be known after the collision occurred - so would not be fair to include in the model)\n",
    "# remove vehicle_leaving_carriageway for same reason above\n",
    "df = df.drop(columns=[\n",
    "    'lsoa_of_casualty', 'lsoa_of_driver', \n",
    "    'age_band_of_casualty', 'age_band_of_driver',\n",
    "    'special_conditions_at_site', 'carriageway_hazards', 'skidding_and_overturning', 'hit_object_in_carriageway', 'hit_object_off_carriageway', 'journey_purpose_of_driver', 'vehicle_leaving_carriageway'\n",
    "])\n",
    "\n",
    "# categorise speed_limit\n",
    "df['speed_limit'] = df['speed_limit'].astype('object')\n",
    "\n",
    "# convert numeric to continuous\n",
    "df[['engine_capacity_cc', 'age_of_casualty', 'age_of_driver']] = df[['engine_capacity_cc', 'age_of_casualty', 'age_of_driver']].replace('Missing', np.nan).astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check there are no duplicates\n",
    "dups = df.duplicated()\n",
    "dups.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary statistics of continuous variables\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# age_of_vehicle has values of -1 -> these must be missing values\n",
    "df['age_of_vehicle'] = df['age_of_vehicle'].replace(-1, np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dealing with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = pd.DataFrame([i for i in zip(df.columns, df.dtypes, df.nunique(), 100 * ((df == \"Missing\") | (df.isnull())).mean()) if i[3] > 0], columns=['column', 'dtype', 'nunique', 'missing %']).sort_values(\"missing %\")\n",
    "missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# viewing frequency distributions for categorical variables with missing values\n",
    "\n",
    "# select only categorical columns\n",
    "categorical_missing = list(missing[missing['dtype'] == 'object']['column'])\n",
    "\n",
    "def plot_freq_dists(df):\n",
    "    # calculate the number of rows and columns for a square layout\n",
    "    num_vars = len(categorical_missing)\n",
    "    cols = math.ceil(math.sqrt(num_vars))\n",
    "    rows = math.ceil(num_vars / cols)\n",
    "\n",
    "    # set up a grid of subplots\n",
    "    fig, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(5 * cols, 5 * rows))\n",
    "    ax = ax.flatten()\n",
    "\n",
    "    # trimming category labels to make the charts easier to read\n",
    "    df_trimmed = df.applymap(lambda x: x[:10] if isinstance(x, str) else x)\n",
    "\n",
    "    # plot frequency distribution for each categorical column\n",
    "    for i, col in enumerate(categorical_missing):\n",
    "        sns.countplot(x=col, data=df_trimmed, ax=ax[i], hue=col, legend=False)\n",
    "        ax[i].set_title(f'Frequency of {col}')\n",
    "        ax[i].xaxis.set_ticks(ax[i].get_xticks()) # add this line to avoid warning\n",
    "        ax[i].set_xticklabels(ax[i].get_xticklabels(), rotation=45, ha='right')\n",
    "        \n",
    "    # hide any unused subplots\n",
    "    for i in range(num_vars, len(ax)):\n",
    "        fig.delaxes(ax[i])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_freq_dists(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- for missing vars we can either simple impute or use a more sophisticated method such as kNN\n",
    "- for categorical vars we can also just leave as a \"Missing\" category\n",
    "\n",
    "The STATS19 data is completed by police at the scene of the incident, so I don't think the missing data is inherently meaningful* for most variables i.e. it seems unlikely that police are deliberately not filling in certain fields because they don't want to share the information. It seems more likely that they simply forgot to fill it in, didn't have enough time, couldn't be bothered, the data was corrupted, etc. However, missing data could still be correlated with the casualty severity - I can imagine that police might be more inclined to spend time filling in all the fields on the form if the collision is not very serious and they have time to spare, however if there's a fatality then filling out a long form might be less of a priority. Since my primary goal is to infer the risk factors for cycle collision severity, I don't want to include these missing categories if they have this 'fake' correlation with casualty severity. I will investigate this further by looking at the correlation of each of these variables with the target, using Cramer's V.\n",
    "\n",
    "*Edit: After reading the guidance to police for filling in the STATS19 form, it does actually seem that missing data can have some meaning. For example, if the sex of the driver is missing then this is likely because there was a hit-and-run incident and so the driver could not be identified. I think this backs up my choice to not add \"Missing\" as a separate category, as a hit-and-run incident is not something that could be known before the collision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "associations(df[categorical_missing + ['casualty_severity']], nom_nom_assoc=\"cramer\", figsize=(15, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at associations again, but with missing values imputed while keeping the category distributions of each variable the same\n",
    "df = df.replace(\"Missing\", np.nan)\n",
    "\n",
    "for col in categorical_missing:\n",
    "    freq_dict = df[col].value_counts(normalize=True).to_dict()\n",
    "    df[col] = df[col].fillna(pd.Series(np.random.choice(list(freq_dict.keys()), p=list(freq_dict.values()), size=len(df))))\n",
    "\n",
    "# does not make sense for propulsion_code to have a value for cyclists\n",
    "df[\"propulsion_code\"] = np.where(df[\"vehicle_type\"] == \"Pedal cycle\", \"Undefined\", df[\"propulsion_code\"])   \n",
    "\n",
    "associations(df[categorical_missing + ['casualty_severity']], nom_nom_assoc=\"cramer\", figsize=(15, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lots of variables are highly associated with each other in the 1st grid but not at all in the 2nd grid - showing that they are only associated because of the missing values. Clearly these fields are either mostly filled in or not filled in at all. For example, in the 1st grid vehicle_left_hand_drive is strongly associated with vehicle_manoeuvre and vehicle_location_restricted_lane - and vehicle_left_hand_drive is also associated with casualty_severity. I suspect that the vehicle_left_hand_drive field is only ever filled in if the driver was doing something dangerous, like a U-turn or sitting in a restricted lane, which would explain why vehicle_left_hand_drive is associated with casualty_severity when I wouldn't really expect it to be.\n",
    "\n",
    "In the 2nd grid the associations make more sense e.g. road_type and junction_detail, propulsion_code and vehicle_type. However, very few of the variables are associated with casualty_severity so I am not sure how useful they will be for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot frequency distributions with imputed missing values\n",
    "plot_freq_dists(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot box plots of each continous variable by target group\n",
    "\n",
    "continuous_missing = list(missing[~(missing['dtype'] == 'object')]['column'])\n",
    "\n",
    "def plot_continuous_vars(plot_type, vars, df):\n",
    "    # set up the number of rows and columns for the grid\n",
    "    num_vars = len(vars)\n",
    "    num_cols = 2\n",
    "    num_rows = (num_vars + num_cols - 1) // num_cols\n",
    "\n",
    "    # create a figure and a grid of subplots\n",
    "    fig, ax = plt.subplots(num_rows, num_cols, figsize=(15, num_rows * 5))\n",
    "    ax = ax.flatten()\n",
    "\n",
    "    for i, var in enumerate(vars):\n",
    "        if plot_type == \"boxplot\":\n",
    "            sns.boxplot(x=\"casualty_severity\", y=var, data=df, ax=ax[i])\n",
    "            ax[i].set_title(f'Boxplot of {var} by {\"casualty_severity\"}')\n",
    "            ax[i].set_xlabel(\"casualty_severity\")\n",
    "            ax[i].set_ylabel(var)\n",
    "        elif plot_type == \"kde\":\n",
    "            sns.histplot(df[var], kde=True, ax=ax[i])\n",
    "            ax[i].set_title(f'Histogram & KDE of {var}')\n",
    "            ax[i].set_xlabel(var)\n",
    "            ax[i].set_ylabel('Density')\n",
    "\n",
    "    # remove empty axes\n",
    "    for i in range(num_vars, num_rows * num_cols):\n",
    "        fig.delaxes(ax[i])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_continuous_vars(\"boxplot\", continuous_missing, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- engine_capacity_cc is much higher for the fatal group - not surprising but didn't expect this trend to be so clear\n",
    "- clear trend in increasing age_of_casualty with increasing casualty severity - I was expecting younger people to be more at risk for fatal accidents due to them taking more risks on the bike, however this is showing the opposite. Perhaps because older people are more likely to take a bad fall, or are less confident on the road\n",
    "- age_of_vehicle distribution looks no different between the different groups, except for a slightly shorter right-hand tail in the fatal group - median looks almost exactly the same between groups. The fatal group is smaller so could just be that the outliers in vehicle age happen to be in the other groups\n",
    "- age_of_driver distribution for the fatal group has larger IQR but smaller range - larger IQR makes sense as we typically think of young or elderly drivers as being more dangerous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_continuous_vars(\"kde\", continuous_missing, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting correlation between all continuous variables and useful categorical variables (based on association grid above)\n",
    "# can use this to see if any variables will be useful for imputing the continuous variables with missing values\n",
    "\n",
    "def is_continuous(series):\n",
    "    return pd.api.types.is_numeric_dtype(series)\n",
    "\n",
    "# function assumes that continuous variables are listed before categorical columns in the 'columns' input\n",
    "def plot_correlation_grid(df, columns, num_cols=3):\n",
    "    num_vars = len(columns)\n",
    "    num_continuous_vars = sum([0 if i in df.select_dtypes(include='object').columns else 1 for i in columns])\n",
    "    num_plots = sum([num_vars - i for i in range(num_continuous_vars)])\n",
    "    num_rows = (num_plots + num_cols - 1) // num_cols\n",
    "\n",
    "    fig, ax = plt.subplots(num_rows, num_cols, figsize=(15, 50))\n",
    "    ax = ax.flatten()\n",
    "    \n",
    "    # get all pairs of variables to plot\n",
    "    plotted_pairs = 0\n",
    "    max_plots = num_rows * num_cols\n",
    "\n",
    "    # axes to delete\n",
    "    delete_axes = []\n",
    "\n",
    "    # trimming category labels to make the charts easier to read\n",
    "    df_trimmed = df.applymap(lambda x: x[:10] if isinstance(x, str) else x)\n",
    "    \n",
    "    for i, var1 in enumerate(columns):\n",
    "        for j, var2 in enumerate(columns):\n",
    "            if i <= j:  # only plot unique combinations\n",
    "                ax_sub = ax[plotted_pairs]\n",
    "                \n",
    "                if var1 == var2: # skip if same variable\n",
    "                    ax_sub.axis('off') \n",
    "                    delete_axes.append(plotted_pairs)\n",
    "                else:\n",
    "                    # if both variables are continuous, create a scatter plot\n",
    "                    if is_continuous(df_trimmed[var1]) and is_continuous(df_trimmed[var2]):\n",
    "                        sns.scatterplot(x=df_trimmed[var1], y=df_trimmed[var2], ax=ax_sub)\n",
    "                    # if one variable is continuous and the other is categorical, create a box plot\n",
    "                    elif is_continuous(df_trimmed[var1]) and not is_continuous(df_trimmed[var2]):\n",
    "                        sns.boxplot(x=df_trimmed[var2], y=df_trimmed[var1], ax=ax_sub)\n",
    "                    elif is_continuous(df_trimmed[var2]) and not is_continuous(df_trimmed[var1]):\n",
    "                        sns.boxplot(x=df_trimmed[var1], y=df_trimmed[var2], ax=ax_sub)\n",
    "                    # skip plots for two categorical variables\n",
    "                    else:\n",
    "                        ax_sub.axis('off')\n",
    "                        delete_axes.append(plotted_pairs)\n",
    "\n",
    "                ax_sub.set_title(f'{var1} vs {var2}')\n",
    "                ax_sub.xaxis.set_ticks(ax_sub.get_xticks()) # add this line to avoid warning\n",
    "                ax_sub.set_xticklabels(ax_sub.get_xticklabels(), rotation=45, ha='right')\n",
    "                plotted_pairs += 1\n",
    "                \n",
    "                if plotted_pairs >= max_plots:\n",
    "                    break\n",
    "        if plotted_pairs >= max_plots:\n",
    "            break\n",
    "\n",
    "    # delete empty axes\n",
    "    for i in delete_axes:\n",
    "        fig.delaxes(ax[i])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_correlation_grid(df, ['age_of_casualty','engine_capacity_cc','age_of_vehicle','age_of_driver',\n",
    "                           'vehicle_type','junction_detail','towing_and_articulation','vehicle_manoeuvre','casualty_imd_decile','pedestrian_crossing_physical_facilities','driver_imd_decile'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- strong correlation between engine_capacity_cc and vehicle_type and towing_and_articulation\n",
    "- some correlation between age_of_vehicle and vehicle_type and towing_and_articulation\n",
    "- age_of_casualty and age_of_driver also correlate to some extent with vehicle_type -> cannot think of a reason why age_of_casualty would be correlated with vehicle_type, so will ignore this correlation\n",
    "\n",
    "I will drop age_of_vehicle since it has little correlation with casualty_severity anyway.\n",
    "\n",
    "Therefore, I will impute each continuous variable based on the categorical variables that they are correlated with (using the median value of the category).\n",
    "\n",
    "I'm using the target variable casualty_severity to impute some of the variables, so will need to make sure I apply the imputation to the training and test data separately in order to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"age_of_vehicle\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating new vehicle_type column because some values of vehicle_type have no values for engine_capacity_cc -> using other types of vehicle that have the most similar engine size\n",
    "df[\"vehicle_type_2\"] = [\n",
    "    \"Goods over 3.5t. and under 7.5t\" if vehicle_type in [\"Agricultural vehicle\", \"Goods vehicle - unknown weight\"]\n",
    "    else \"Motorcycle 50cc and under\" if vehicle_type == \"Electric motorcycle\"\n",
    "    else \"Motorcycle 125cc and under\" if vehicle_type == \"Motorcycle - unknown cc\"\n",
    "    else \"Car\" if vehicle_type == \"Unknown vehicle type (self rep only)\"\n",
    "    else vehicle_type\n",
    "    for vehicle_type in df[\"vehicle_type\"]\n",
    "]\n",
    "\n",
    "# imputing continuous variables\n",
    "# first imputation should capture most nulls\n",
    "# second imputation should capture any remaining nulls (remaining because there were no values in the lookup group)\n",
    "impute_dict_1 = {\n",
    "    \"engine_capacity_cc\": [\"vehicle_type_2\"],\n",
    "    \"age_of_casualty\": [\"casualty_severity\"],\n",
    "    \"age_of_driver\": [\"vehicle_type\", \"casualty_severity\"]\n",
    "}\n",
    "impute_dict_2 = {\n",
    "    \"engine_capacity_cc\": [\"vehicle_type_2\"],\n",
    "    \"age_of_casualty\": [\"casualty_severity\"],\n",
    "    \"age_of_driver\": [\"casualty_severity\"]\n",
    "}\n",
    "\n",
    "def impute_continuous_vars(df, impute_dict):\n",
    "    for var in impute_dict:\n",
    "        lookup_df = df.groupby(impute_dict[var])[var].median()\n",
    "        df = pd.merge(df, lookup_df, how=\"left\", on=impute_dict[var], suffixes=[\"\", \"_y\"])\n",
    "        df[var] = np.where(df[var].isna(), df[var + '_y'], df[var])\n",
    "        df = df.drop(columns=[var + '_y'])\n",
    "\n",
    "    return df\n",
    "\n",
    "df_1 = impute_continuous_vars(df, impute_dict_1)\n",
    "df_2 = impute_continuous_vars(df_1, impute_dict_2)\n",
    "df = df_2\n",
    "\n",
    "# \"Pedal cycle\" has no values for engine_capacity_cc for obvious reasons\n",
    "# assume average cyclist can push 100W â‰ˆ 0.13 horsepower -> horsepower of standard car ~200 -> cyclist horsepower 0.2/200=0.00065 of a car -> set engine_capacity_cc of \"Pedal cycle\" to 0.00125 that of a car\n",
    "engine_capacity_car = df.groupby(\"vehicle_type\")[\"engine_capacity_cc\"].median().loc[\"Car\"]\n",
    "df[\"engine_capacity_cc\"] = np.where(df[\"vehicle_type\"] == \"Pedal cycle\", 0.00065 * engine_capacity_car, df[\"engine_capacity_cc\"])\n",
    "\n",
    "df = df.drop(columns=[\"vehicle_type_2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check there are no more missing values\n",
    "assert sum(((df.isna()) | (df == \"Missing\")).any()) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating date and time features\n",
    "df['time_period'] = (df.time.str.slice(start=0, stop=2).astype('int') // 4)\n",
    "df['time_period'] = (df['time_period'] * 4).astype('str') + ':00 - ' + ((df['time_period'] + 1) * 4).astype('str') + ':00'\n",
    "\n",
    "df['date'] = pd.to_datetime(df.date)\n",
    "df['month'] = df['date'].dt.month\n",
    "df['season'] = [\n",
    "    'spring' if month in [3, 4, 5]\n",
    "    else 'summer' if month in [6, 7, 8]\n",
    "    else 'autumn' if month in [9, 10, 11]\n",
    "    else 'winter'\n",
    "    for month\n",
    "    in df['month']\n",
    "]\n",
    "\n",
    "df = df.drop(columns=['date', 'month'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transforming variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot continuous variable distributions\n",
    "continuous_vars = [col for col in list(df.select_dtypes(exclude='object').columns) if col not in ['longitude', 'latitude']]\n",
    "plot_continuous_vars('kde', continuous_vars, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PRECTOTCORR and engine_capacity_cc are highly right skewed -> could apply a log or quantile transformation to these, but will leave for now\n",
    "- age_of_driver distribution has been significantly affected by the imputation -> might experiment with dropping this variable as it does not seem very predictive anyway\n",
    "\n",
    "Below I am normalising all continuous variables to fit into a [0, 1] range (I don't think this is necessary for the algorithms I'm using, but shouldn't do any harm), and one-hot encoding all categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise continuous variables\n",
    "scaler = MinMaxScaler()\n",
    "df[continuous_vars] = scaler.fit_transform(df[continuous_vars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode categorical variables\n",
    "categorical_vars = [col for col in list(df.select_dtypes(include='object').columns) if col not in ['date', 'time']]\n",
    "df = pd.get_dummies(df, columns=categorical_vars, drop_first=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cycleCollisions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
